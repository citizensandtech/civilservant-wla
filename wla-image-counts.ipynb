{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "1. verify\n",
    "Hi Max, I wrote code to parse the mediacounts file, and I was disappointed to see how incomplete the data is.# The webrequests tables only had records for 608,991 images, and only 200 out of the 39,114 WikiLovesAfrica images.\n",
    "\n",
    "\n",
    "## Notes:\n",
    "1. Catgories: Images_from_Wiki_Loves_Africa_2017, have many subcategories\n",
    "2. Is nathan searching for the usage on non-commons wikis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2, codecs, csv, zipfile, glob, re, csv, pymysql, os, datetime, urllib, sys\n",
    "import csv, json, argparse, sys, datetime, os, re, time, bz2\n",
    "from collections import defaultdict, Counter\n",
    "from dateutil import parser\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import ProgrammingError\n",
    "from pymysql.err import InternalError, OperationalError\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are taken from\n",
    "# http://dumps.wikimedia.org/other/mediacounts/README.txt\n",
    "FIELDS = [\n",
    "    \"filename\",\n",
    "    \"total_response_bytes\",\n",
    "    \"total_transfers\",\n",
    "    \"total_transfers_raw\",\n",
    "    \"total_transfers_audio\",\n",
    "    \"reserved6\",\n",
    "    \"reserved7\",\n",
    "    \"total_transfers_image\",\n",
    "    \"total_transfers_image_0x199\",\n",
    "    \"total_transfers_image_200x399\",\n",
    "    \"total_transfers_image_400x599\",\n",
    "    \"total_transfers_image_600x799\",\n",
    "    \"total_transfers_image_800x999\",\n",
    "    \"total_transfers_image_1000plus\",\n",
    "    \"reserved15\",\n",
    "    \"reserved16\",\n",
    "    \"total_transfers_movie\",\n",
    "    \"total_transfers_movie_0x239\",\n",
    "    \"total_transfers_movie_240x479\",\n",
    "    \"total_transfers_movie_480plus\",\n",
    "    \"reserved21\",\n",
    "    \"reserved22\",\n",
    "    \"total_transfers_refer_wmf\",\n",
    "    \"total_transfers_refer_nonwmf\",\n",
    "    \"total_transfers_refer_invalid\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_database = \"commonswiki_p\"\n",
    "\n",
    "constr = 'mysql+pymysql://{user}:{pwd}@{host}/DB?charset=utf8'.format(user=os.environ['MYSQL_USERNAME'],\n",
    "                                                      pwd=os.environ['MYSQL_PASSWORD'],\n",
    "                                                      host=os.environ['MYSQL_HOST'],\n",
    "                                                                     use_unicode=True)\n",
    "\n",
    "con = create_engine(constr, encoding='utf-8')\n",
    "\n",
    "def use_commons_exec():\n",
    "    con.execute(f'use commonswiki_p;')\n",
    "    \n",
    "def wmftimestamp(bytestring):\n",
    "    if bytestring:\n",
    "        s = bytestring.decode('utf-8')\n",
    "        return dt.strptime(s, '%Y%m%d%H%M%S')\n",
    "    else:\n",
    "        return bytestring\n",
    "    \n",
    "\n",
    "def decode_or_nan(b):\n",
    "    return b.decode('utf-8') if b else float('nan')\n",
    "    \n",
    "use_commons_exec()\n",
    "\n",
    "wla_years = [2014,2015,2016,2017]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wla_image_titles_from_year(year):\n",
    "    year_category = f\"Images_from_Wiki_Loves_Africa_{year}\"\n",
    "    print(f\"Year category is: {year_category}\")\n",
    "    year_cat_sql = f'''        SELECT img_user_text, img_name \n",
    "            FROM image, page, categorylinks\n",
    "            WHERE page.page_id=categorylinks.cl_from \n",
    "               AND image.img_name = page.page_title\n",
    "               AND .categorylinks.cl_to = \"{year_category}\"'''\n",
    "    use_commons_exec()\n",
    "    year_cat_df = pd.read_sql(year_cat_sql, con)\n",
    "    year_cat_df['img_user_text'] = year_cat_df['img_user_text'].apply(decode_or_nan)\n",
    "    year_cat_df['img_name'] = year_cat_df['img_name'].apply(decode_or_nan)\n",
    "    year_cat_df['year']=year\n",
    "    print(f\"Number results are: {len(year_cat_df)}\")\n",
    "    return year_cat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    img_df = pd.read_pickle('cache/img_df.pickle')\n",
    "except FileNotFoundError:\n",
    "    wla_image_titles_dfs = [get_wla_image_titles_from_year(year) for year in wla_years]\n",
    "    img_df = pd.concat(wla_image_titles_dfs)\n",
    "    img_df.to_pickle('cache/img_df.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## borrowing from https://github.com/hay/wiki-tools/blob/master/etc/mediacounts-stats.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load https://raw.githubusercontent.com/hay/wiki-tools/master/etc/mediacounts-stats.py\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "\n",
    "def process(datafile, query):\n",
    "    match_rows = []\n",
    "#     log(\"Doing %s\" % datafile)\n",
    "    if datafile.endswith(\"bz2\"):\n",
    "        tsvfile = bz2.open(datafile, \"rt\")\n",
    "    else:\n",
    "        tsvfile = open(datafile)\n",
    "    tsvfilesize = os.path.getsize(datafile)\n",
    "    # Actually benefit from the generator, e.g. batch\n",
    "    query = frozenset(query)\n",
    "\n",
    "    for index, line in enumerate(tsvfile):\n",
    "        row = line.split(\"\\t\")\n",
    "        \n",
    "        filename = row[0].split(\"/\")[-1]\n",
    "        if filename not in query:\n",
    "            continue\n",
    "\n",
    "        clean_row = [e.strip() for e in row]\n",
    "        row_series = pd.Series(clean_row)\n",
    "        match_rows.append(row_series)\n",
    "    tsvfile.close()\n",
    "    return match_rows\n",
    "\n",
    "\n",
    "def parse_mediacounts(tsvs, target_filenames):\n",
    "    print(f'Looking at {len(tsvs)} TSVs')\n",
    "    query = target_filenames\n",
    "    log(\"Searching statistics for %d files\" % len(query))\n",
    "    match_rows_dfs = []\n",
    "    for tsv in tsvs:\n",
    "        cache_key = os.path.basename(tsv)\n",
    "        cache_file = f'cache/{cache_key}.result.pickle'\n",
    "        if os.path.exists(cache_file):\n",
    "            match_rows_df = pd.read_pickle(cache_file)\n",
    "            match_rows_dfs.append(match_rows_df)\n",
    "            sys.stdout.write('c')\n",
    "        else:\n",
    "            now = time.time()\n",
    "            match_rows = process(tsv, query)\n",
    "            match_rows_df = pd.DataFrame(match_rows)\n",
    "            match_rows_df.columns = FIELDS\n",
    "            match_rows_df['tsv_name'] = cache_key\n",
    "            match_rows_df.to_pickle(cache_file)\n",
    "            match_rows_dfs.append(match_rows_df)\n",
    "            log(\"%s took %s seconds\" % (tsv, round(time.time() - now, 2)))\n",
    "    return match_rows_dfs\n",
    "\n",
    "\n",
    "def make_wla_views_counts_df(year):\n",
    "    try:\n",
    "        target_filenames = frozenset(img_df['img_name'].values)\n",
    "    except NameError:\n",
    "        print(\"please compute img_df first\")\n",
    "        return\n",
    "    \n",
    "    tsvs_base = \"/public/dumps/public/other/mediacounts/daily/\"\n",
    "    tsvs_year = os.path.join(tsvs_base, str(year))\n",
    "    tsvs_all_year_files = os.listdir(tsvs_year)\n",
    "    tsvs_rel = [path for path in tsvs_all_year_files if '.tsv.bz2' in path]\n",
    "    tsvs = [os.path.join(tsvs_year, f) for f in tsvs_rel]\n",
    "    tsvs = list(reversed(sorted(tsvs)))\n",
    "    match_rows_tsv = parse_mediacounts(tsvs, target_filenames)\n",
    "    \n",
    "    counts_df = pd.concat(match_rows_tsv)\n",
    "    outfile = f'output/wla_mediacounts_{year}.csv'\n",
    "    print(f'saving outfile: {outfile}')\n",
    "    counts_df.to_csv(outfile, index=False)\n",
    "    return counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in wla_years:\n",
    "    make_wla_views_counts_df(year+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking at 365 TSVs\n",
      "Searching statistics for 39113 files\n",
      "ccccccccccccccccccccccccccccccccccccccccccccc/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-16.v00.tsv.bz2 took 216.29 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-15.v00.tsv.bz2 took 232.34 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-14.v00.tsv.bz2 took 267.25 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-13.v00.tsv.bz2 took 228.55 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-12.v00.tsv.bz2 took 238.78 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-11.v00.tsv.bz2 took 223.66 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-10.v00.tsv.bz2 took 217.47 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-09.v00.tsv.bz2 took 231.26 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-08.v00.tsv.bz2 took 233.48 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-07.v00.tsv.bz2 took 253.56 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-06.v00.tsv.bz2 took 230.75 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-05.v00.tsv.bz2 took 237.0 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-04.v00.tsv.bz2 took 233.53 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-03.v00.tsv.bz2 took 224.98 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-02.v00.tsv.bz2 took 236.62 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-11-01.v00.tsv.bz2 took 241.6 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-31.v00.tsv.bz2 took 232.77 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-30.v00.tsv.bz2 took 279.76 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-29.v00.tsv.bz2 took 233.86 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-28.v00.tsv.bz2 took 233.82 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-27.v00.tsv.bz2 took 228.11 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-26.v00.tsv.bz2 took 230.9 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-25.v00.tsv.bz2 took 241.86 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-24.v00.tsv.bz2 took 250.91 seconds\n",
      "/public/dumps/public/other/mediacounts/daily/2018/mediacounts.2018-10-23.v00.tsv.bz2 took 270.67 seconds\n"
     ]
    }
   ],
   "source": [
    "make_wla_views_counts_df(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
